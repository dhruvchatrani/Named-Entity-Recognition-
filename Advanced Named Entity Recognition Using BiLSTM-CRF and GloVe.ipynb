{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf85ffc2-88a8-4090-aeb5-b829ddf41314",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-14 20:39:50,192 - INFO - Using device: cuda\n",
      "2025-08-14 20:39:50,196 - INFO - Loading data...\n",
      "2025-08-14 20:39:50,197 - INFO - Loading embeddings from glove.6B.100d.txt\n",
      "2025-08-14 20:39:50,375 - INFO - Loaded 10000 embeddings\n",
      "2025-08-14 20:39:50,378 - INFO - Reading data from train.txt\n",
      "2025-08-14 20:39:50,464 - INFO - Loaded 14987 sentences from train.txt\n",
      "2025-08-14 20:39:50,465 - INFO - Reading data from dev.txt\n",
      "2025-08-14 20:39:50,488 - INFO - Loaded 3466 sentences from dev.txt\n",
      "2025-08-14 20:39:50,489 - INFO - Reading data from test.txt\n",
      "2025-08-14 20:39:50,516 - INFO - Loaded 3684 sentences from test.txt\n",
      "2025-08-14 20:39:50,517 - INFO - Augmenting data...\n",
      "2025-08-14 20:39:50,518 - INFO - Augmenting data...\n",
      "Augmenting sentences: 100%|█████████████████████████████| 14987/14987 [00:02<00:00, 6749.96it/s]\n",
      "2025-08-14 20:39:52,894 - INFO - Augmented data size: 14987 sentences\n",
      "2025-08-14 20:40:39,463 - INFO - Training strategy_a with lr=0.001, dropout=0.3\n",
      "2025-08-14 20:40:39,463 - INFO - Creating embedding layer for strategy strategy_a\n",
      "2025-08-14 20:40:39,516 - INFO - Starting training...\n",
      "2025-08-14 20:40:39,517 - INFO - Training with 29974 samples, 468 batches per epoch\n",
      "2025-08-14 20:40:39,521 - INFO - Sequence lengths: min=1, max=113, mean=13.65\n",
      "Epoch 1/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 32.97it/s]\n",
      "2025-08-14 20:41:29,137 - INFO - Epoch 1/5\n",
      "2025-08-14 20:41:29,138 - INFO - Loss: 337.8719\n",
      "2025-08-14 20:41:29,138 - INFO - Train - Precision: 0.8646, Recall: 0.8646, F1: 0.8646\n",
      "2025-08-14 20:41:29,139 - INFO - Valid - Precision: 0.8632, Recall: 0.8632, F1: 0.8632\n",
      "2025-08-14 20:41:29,156 - INFO - Saved checkpoint to checkpoints/epoch_1.pt\n",
      "Epoch 2/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 32.78it/s]\n",
      "2025-08-14 20:42:18,569 - INFO - Epoch 2/5\n",
      "2025-08-14 20:42:18,569 - INFO - Loss: 211.5010\n",
      "2025-08-14 20:42:18,570 - INFO - Train - Precision: 0.8625, Recall: 0.8625, F1: 0.8625\n",
      "2025-08-14 20:42:18,571 - INFO - Valid - Precision: 0.8608, Recall: 0.8608, F1: 0.8608\n",
      "2025-08-14 20:42:18,589 - INFO - Saved checkpoint to checkpoints/epoch_2.pt\n",
      "Epoch 3/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 33.16it/s]\n",
      "2025-08-14 20:43:08,389 - INFO - Epoch 3/5\n",
      "2025-08-14 20:43:08,390 - INFO - Loss: 179.1949\n",
      "2025-08-14 20:43:08,390 - INFO - Train - Precision: 0.8678, Recall: 0.8678, F1: 0.8678\n",
      "2025-08-14 20:43:08,391 - INFO - Valid - Precision: 0.8625, Recall: 0.8625, F1: 0.8625\n",
      "2025-08-14 20:43:08,412 - INFO - Saved checkpoint to checkpoints/epoch_3.pt\n",
      "2025-08-14 20:43:08,413 - INFO - Early stopping at epoch 3\n",
      "2025-08-14 20:43:12,152 - INFO - strategy_a results on test set:\n",
      "2025-08-14 20:43:12,153 - INFO - precision - 0.8579\n",
      "2025-08-14 20:43:12,154 - INFO - recall - 0.8579\n",
      "2025-08-14 20:43:12,154 - INFO - f1 - 0.8579\n",
      "2025-08-14 20:43:12,154 - INFO - f0.5 - 0.8579\n",
      "2025-08-14 20:43:12,156 - INFO - Saving visualization to strategy_a_lr0.001_drop0.3.png\n",
      "2025-08-14 20:43:12,346 - INFO - Training strategy_b with lr=0.001, dropout=0.3\n",
      "2025-08-14 20:43:12,347 - INFO - Creating embedding layer for strategy strategy_b\n",
      "2025-08-14 20:43:12,445 - INFO - Starting training...\n",
      "2025-08-14 20:43:12,447 - INFO - Training with 29974 samples, 468 batches per epoch\n",
      "2025-08-14 20:43:12,451 - INFO - Sequence lengths: min=1, max=113, mean=13.65\n",
      "Epoch 1/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 32.20it/s]\n",
      "2025-08-14 20:44:02,228 - INFO - Epoch 1/5\n",
      "2025-08-14 20:44:02,229 - INFO - Loss: 242.2536\n",
      "2025-08-14 20:44:02,230 - INFO - Train - Precision: 0.9724, Recall: 0.9724, F1: 0.9724\n",
      "2025-08-14 20:44:02,230 - INFO - Valid - Precision: 0.9300, Recall: 0.9300, F1: 0.9300\n",
      "2025-08-14 20:44:02,246 - INFO - Saved checkpoint to checkpoints/epoch_1.pt\n",
      "Epoch 2/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 32.02it/s]\n",
      "2025-08-14 20:44:52,158 - INFO - Epoch 2/5\n",
      "2025-08-14 20:44:52,159 - INFO - Loss: 39.2640\n",
      "2025-08-14 20:44:52,159 - INFO - Train - Precision: 0.9895, Recall: 0.9895, F1: 0.9895\n",
      "2025-08-14 20:44:52,160 - INFO - Valid - Precision: 0.9427, Recall: 0.9427, F1: 0.9427\n",
      "2025-08-14 20:44:52,175 - INFO - Saved checkpoint to checkpoints/epoch_2.pt\n",
      "Epoch 3/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 32.03it/s]\n",
      "2025-08-14 20:45:42,174 - INFO - Epoch 3/5\n",
      "2025-08-14 20:45:42,175 - INFO - Loss: 13.6181\n",
      "2025-08-14 20:45:42,175 - INFO - Train - Precision: 0.9937, Recall: 0.9937, F1: 0.9937\n",
      "2025-08-14 20:45:42,175 - INFO - Valid - Precision: 0.9410, Recall: 0.9410, F1: 0.9410\n",
      "2025-08-14 20:45:42,190 - INFO - Saved checkpoint to checkpoints/epoch_3.pt\n",
      "Epoch 4/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 31.69it/s]\n",
      "2025-08-14 20:46:32,585 - INFO - Epoch 4/5\n",
      "2025-08-14 20:46:32,586 - INFO - Loss: 7.0171\n",
      "2025-08-14 20:46:32,587 - INFO - Train - Precision: 0.9958, Recall: 0.9958, F1: 0.9958\n",
      "2025-08-14 20:46:32,587 - INFO - Valid - Precision: 0.9451, Recall: 0.9451, F1: 0.9451\n",
      "2025-08-14 20:46:32,599 - INFO - Saved checkpoint to checkpoints/epoch_4.pt\n",
      "Epoch 5/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 32.29it/s]\n",
      "2025-08-14 20:47:22,343 - INFO - Epoch 5/5\n",
      "2025-08-14 20:47:22,344 - INFO - Loss: 3.8124\n",
      "2025-08-14 20:47:22,345 - INFO - Train - Precision: 0.9971, Recall: 0.9971, F1: 0.9971\n",
      "2025-08-14 20:47:22,345 - INFO - Valid - Precision: 0.9466, Recall: 0.9466, F1: 0.9466\n",
      "2025-08-14 20:47:22,356 - INFO - Saved checkpoint to checkpoints/epoch_5.pt\n",
      "2025-08-14 20:47:26,055 - INFO - strategy_b results on test set:\n",
      "2025-08-14 20:47:26,056 - INFO - precision - 0.8982\n",
      "2025-08-14 20:47:26,056 - INFO - recall - 0.8982\n",
      "2025-08-14 20:47:26,056 - INFO - f1 - 0.8982\n",
      "2025-08-14 20:47:26,057 - INFO - f0.5 - 0.8982\n",
      "2025-08-14 20:47:26,059 - INFO - Saving visualization to strategy_b_lr0.001_drop0.3.png\n",
      "2025-08-14 20:47:26,241 - INFO - Training strategy_c with lr=0.001, dropout=0.3\n",
      "2025-08-14 20:47:26,241 - INFO - Creating embedding layer for strategy strategy_c\n",
      "2025-08-14 20:48:05,176 - INFO - Starting training...\n",
      "2025-08-14 20:48:05,178 - INFO - Training with 29974 samples, 468 batches per epoch\n",
      "2025-08-14 20:48:05,182 - INFO - Sequence lengths: min=1, max=113, mean=13.65\n",
      "Epoch 1/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 31.99it/s]\n",
      "2025-08-14 20:48:55,120 - INFO - Epoch 1/5\n",
      "2025-08-14 20:48:55,120 - INFO - Loss: 283.1375\n",
      "2025-08-14 20:48:55,121 - INFO - Train - Precision: 0.9671, Recall: 0.9671, F1: 0.9671\n",
      "2025-08-14 20:48:55,121 - INFO - Valid - Precision: 0.9275, Recall: 0.9275, F1: 0.9275\n",
      "2025-08-14 20:48:55,137 - INFO - Saved checkpoint to checkpoints/epoch_1.pt\n",
      "Epoch 2/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 31.91it/s]\n",
      "2025-08-14 20:49:45,283 - INFO - Epoch 2/5\n",
      "2025-08-14 20:49:45,283 - INFO - Loss: 44.4956\n",
      "2025-08-14 20:49:45,284 - INFO - Train - Precision: 0.9912, Recall: 0.9912, F1: 0.9912\n",
      "2025-08-14 20:49:45,284 - INFO - Valid - Precision: 0.9368, Recall: 0.9368, F1: 0.9368\n",
      "2025-08-14 20:49:45,299 - INFO - Saved checkpoint to checkpoints/epoch_2.pt\n",
      "Epoch 3/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 32.10it/s]\n",
      "2025-08-14 20:50:35,532 - INFO - Epoch 3/5\n",
      "2025-08-14 20:50:35,532 - INFO - Loss: 13.1802\n",
      "2025-08-14 20:50:35,533 - INFO - Train - Precision: 0.9964, Recall: 0.9964, F1: 0.9964\n",
      "2025-08-14 20:50:35,533 - INFO - Valid - Precision: 0.9504, Recall: 0.9504, F1: 0.9504\n",
      "2025-08-14 20:50:35,548 - INFO - Saved checkpoint to checkpoints/epoch_3.pt\n",
      "Epoch 4/5: 100%|██████████████████████████████████████████████| 469/469 [00:14<00:00, 32.48it/s]\n",
      "2025-08-14 20:51:25,386 - INFO - Epoch 4/5\n",
      "2025-08-14 20:51:25,387 - INFO - Loss: 6.9776\n",
      "2025-08-14 20:51:25,387 - INFO - Train - Precision: 0.9979, Recall: 0.9979, F1: 0.9979\n",
      "2025-08-14 20:51:25,388 - INFO - Valid - Precision: 0.9499, Recall: 0.9499, F1: 0.9499\n",
      "2025-08-14 20:51:25,403 - INFO - Saved checkpoint to checkpoints/epoch_4.pt\n",
      "Epoch 5/5: 100%|██████████████████████████████████████████████| 469/469 [00:15<00:00, 30.79it/s]\n",
      "2025-08-14 20:52:15,848 - INFO - Epoch 5/5\n",
      "2025-08-14 20:52:15,848 - INFO - Loss: 3.7045\n",
      "2025-08-14 20:52:15,848 - INFO - Train - Precision: 0.9983, Recall: 0.9983, F1: 0.9983\n",
      "2025-08-14 20:52:15,849 - INFO - Valid - Precision: 0.9489, Recall: 0.9489, F1: 0.9489\n",
      "2025-08-14 20:52:15,864 - INFO - Saved checkpoint to checkpoints/epoch_5.pt\n",
      "2025-08-14 20:52:15,864 - INFO - Early stopping at epoch 5\n",
      "2025-08-14 20:52:19,558 - INFO - strategy_c results on test set:\n",
      "2025-08-14 20:52:19,559 - INFO - precision - 0.9073\n",
      "2025-08-14 20:52:19,559 - INFO - recall - 0.9073\n",
      "2025-08-14 20:52:19,559 - INFO - f1 - 0.9073\n",
      "2025-08-14 20:52:19,560 - INFO - f0.5 - 0.9073\n",
      "2025-08-14 20:52:19,561 - INFO - Saving visualization to strategy_c_lr0.001_drop0.3.png\n",
      "2025-08-14 20:52:19,746 - INFO - Best model: strategy_c with F1 score: 0.9073\n",
      "2025-08-14 20:52:19,761 - INFO - Saved best model to best_model.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "try:\n",
    "    from torchcrf import CRF\n",
    "except ImportError:\n",
    "    print(\"Error: torchcrf not found. Please install it using 'pip install torchcrf' in the correct environment.\")\n",
    "    exit(1)\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "import random\n",
    "import copy\n",
    "import os\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Check for CUDA\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Utility functions\n",
    "def load_embedding_dict(path, max_words=10000):\n",
    "    \"\"\"Load GloVe embeddings, limiting to max_words for efficiency.\"\"\"\n",
    "    logger.info(f\"Loading embeddings from {path}\")\n",
    "    embeddings = {}\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if i >= max_words:\n",
    "                    break\n",
    "                values = line.strip().split()\n",
    "                word = values[0]\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[word] = vector\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Embedding file {path} not found.\")\n",
    "        exit(1)\n",
    "    logger.info(f\"Loaded {len(embeddings)} embeddings\")\n",
    "    return embeddings\n",
    "\n",
    "def read_ner_data_from_connl(file_path):\n",
    "    \"\"\"Read NER data in CoNLL format.\"\"\"\n",
    "    logger.info(f\"Reading data from {file_path}\")\n",
    "    words, tags = [], []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            sentence, sentence_tags = [], []\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    parts = line.strip().split()\n",
    "                    if len(parts) < 4:\n",
    "                        logger.warning(f\"Skipping malformed line in {file_path}: {line.strip()}\")\n",
    "                        continue\n",
    "                    word, _, _, tag = parts\n",
    "                    sentence.append(word)\n",
    "                    sentence_tags.append(tag)\n",
    "                else:\n",
    "                    if sentence:\n",
    "                        words.append(sentence)\n",
    "                        tags.append(sentence_tags)\n",
    "                        sentence, sentence_tags = [], []\n",
    "            if sentence:\n",
    "                words.append(sentence)\n",
    "                tags.append(sentence_tags)\n",
    "        logger.info(f\"Loaded {len(words)} sentences from {file_path}\")\n",
    "        if not words:\n",
    "            logger.error(f\"No data loaded from {file_path}. Check file format or path.\")\n",
    "            exit(1)\n",
    "        return words, tags\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Data file {file_path} not found.\")\n",
    "        exit(1)\n",
    "\n",
    "def augment_data(words, tags, glove, augment_factor=0.05):\n",
    "    \"\"\"Augment data by replacing words with similar GloVe embeddings.\"\"\"\n",
    "    logger.info(\"Augmenting data...\")\n",
    "    augmented_words, augmented_tags = copy.deepcopy(words), copy.deepcopy(tags)\n",
    "    glove_words = list(glove.keys())\n",
    "    frequent_words = glove_words[:min(5000, len(glove_words))]\n",
    "    word_vectors = np.array([glove[w] for w in frequent_words])\n",
    "    \n",
    "    for i in tqdm(range(len(words)), desc=\"Augmenting sentences\"):\n",
    "        sentence = words[i]\n",
    "        for j, word in enumerate(sentence):\n",
    "            if random.random() < augment_factor:\n",
    "                if word in glove:\n",
    "                    word_vec = glove[word]\n",
    "                    similarities = np.dot(word_vectors, word_vec) / (\n",
    "                        np.linalg.norm(word_vectors, axis=1) * np.linalg.norm(word_vec)\n",
    "                    )\n",
    "                    top_idx = np.argmax(similarities)\n",
    "                    if similarities[top_idx] > 0.7:\n",
    "                        augmented_words[i][j] = frequent_words[top_idx]\n",
    "    logger.info(f\"Augmented data size: {len(augmented_words)} sentences\")\n",
    "    return augmented_words, augmented_tags\n",
    "\n",
    "# Indexer class\n",
    "class Indexer:\n",
    "    def __init__(self, elements):\n",
    "        self.element_to_index = {'<PAD>': 0, '<UNK>': 1}\n",
    "        self.index_to_element = {0: '<PAD>', 1: '<UNK>'}\n",
    "        for element in set(sum(elements, [])):\n",
    "            if element not in self.element_to_index:\n",
    "                index = len(self.element_to_index)\n",
    "                self.element_to_index[element] = index\n",
    "                self.index_to_element[index] = element\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.element_to_index)\n",
    "    \n",
    "    def elements_to_index(self, elements):\n",
    "        return [[self.element_to_index.get(e, 1) for e in seq] for seq in elements]\n",
    "    \n",
    "    def get_element_to_index_dict(self):\n",
    "        return self.element_to_index\n",
    "\n",
    "# Embedding fabric\n",
    "class EmbeddingFabric:\n",
    "    @staticmethod\n",
    "    def get_embedding_layer(word_indexer, glove, strategy):\n",
    "        \"\"\"Create embedding layer based on strategy.\"\"\"\n",
    "        logger.info(f\"Creating embedding layer for strategy {strategy}\")\n",
    "        vocab_size = word_indexer.size()\n",
    "        embedding_matrix = np.zeros((vocab_size, 100))\n",
    "        for word, idx in word_indexer.element_to_index.items():\n",
    "            if strategy == 'strategy_a':\n",
    "                embedding_matrix[idx] = glove.get(word, np.zeros(100))\n",
    "            elif strategy == 'strategy_b':\n",
    "                embedding_matrix[idx] = glove.get(word, np.random.normal(0, 0.1, 100))\n",
    "            else:\n",
    "                embedding_matrix[idx] = glove.get(word, np.mean(list(glove.values()), axis=0))\n",
    "        embedding_layer = nn.Embedding(vocab_size, 100)\n",
    "        embedding_layer.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        embedding_layer.weight.requires_grad = (strategy != 'strategy_a')\n",
    "        return embedding_layer\n",
    "\n",
    "# Metrics handler\n",
    "class MetricsHandler:\n",
    "    def __init__(self, labels):\n",
    "        self.labels = labels\n",
    "        self.metrics_dict = defaultdict(list)\n",
    "    \n",
    "    def update(self, predictions, true_vals):\n",
    "        \"\"\"Update metrics for a batch.\"\"\"\n",
    "        for pred, true in zip(predictions, true_vals):\n",
    "            correct = sum(1 for p, t in zip(pred, true) if p == t and t != 0)\n",
    "            pred_pos = sum(1 for p in pred if p != 0)\n",
    "            true_pos = sum(1 for t in true if t != 0)\n",
    "            precision = correct / pred_pos if pred_pos else 0\n",
    "            recall = correct / true_pos if true_pos else 0\n",
    "            self.metrics_dict['precision'].append(precision)\n",
    "            self.metrics_dict['recall'].append(recall)\n",
    "            self.metrics_dict['f1'].append(2 * precision * recall / (precision + recall) if precision + recall else 0)\n",
    "            self.metrics_dict['f0.5'].append((1 + 0.5**2) * precision * recall / (0.5**2 * precision + recall) if precision + recall else 0)\n",
    "    \n",
    "    def collect(self):\n",
    "        \"\"\"Aggregate metrics.\"\"\"\n",
    "        for metric in self.metrics_dict:\n",
    "            if self.metrics_dict[metric]:\n",
    "                self.metrics_dict[metric] = [np.mean(self.metrics_dict[metric])]\n",
    "            else:\n",
    "                self.metrics_dict[metric] = [0.0]\n",
    "    \n",
    "    def get_metrics(self):\n",
    "        return self.metrics_dict\n",
    "\n",
    "# BiLSTM-CRF Model\n",
    "class BiLSTMCRFTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, tagset_size, embedding_layer, dropout=0.3):\n",
    "        super(BiLSTMCRFTagger, self).__init__()\n",
    "        self.embedding = embedding_layer\n",
    "        self.bilstm = nn.LSTM(embedding_dim, hidden_dim // 2, num_layers=2, \n",
    "                             bidirectional=True, batch_first=True, dropout=dropout)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "        self.crf = CRF(tagset_size, batch_first=True)\n",
    "    \n",
    "    def forward(self, sentence, tags=None, mask=None):\n",
    "        embeds = self.embedding(sentence)\n",
    "        lstm_out, _ = self.bilstm(embeds)\n",
    "        emissions = self.hidden2tag(lstm_out)\n",
    "        if tags is not None:\n",
    "            return -self.crf(emissions, tags, mask=mask)\n",
    "        return self.crf.decode(emissions, mask=mask)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, optimizer, scheduler, data_dict, batch_size, words_indexer, tags_indexer, \n",
    "                metric_handler, valid_metric, num_epochs=5, patience=2):\n",
    "    \"\"\"Train the model with early stopping and checkpointing.\"\"\"\n",
    "    logger.info(\"Starting training...\")\n",
    "    model.to(device)\n",
    "    train_data, train_tags = data_dict['train']\n",
    "    valid_data, valid_tags = data_dict['dev']\n",
    "    \n",
    "    # Validate data size\n",
    "    if not train_data:\n",
    "        logger.error(\"Training data is empty. Check data loading.\")\n",
    "        exit(1)\n",
    "    if len(train_data) < batch_size:\n",
    "        logger.warning(f\"Training data size ({len(train_data)}) is smaller than batch size ({batch_size}). Adjusting batch size.\")\n",
    "        batch_size = max(1, len(train_data) // 2)\n",
    "    \n",
    "    num_batches = max(1, len(train_data) // batch_size)\n",
    "    logger.info(f\"Training with {len(train_data)} samples, {num_batches} batches per epoch\")\n",
    "    \n",
    "    # Log sequence lengths\n",
    "    seq_lengths = [len(seq) for seq in train_data]\n",
    "    logger.info(f\"Sequence lengths: min={min(seq_lengths)}, max={max(seq_lengths)}, mean={np.mean(seq_lengths):.2f}\")\n",
    "    \n",
    "    losses = []\n",
    "    best_f1 = 0\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    checkpoint_dir = 'checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        metric_handler.metrics_dict.clear()\n",
    "        train_data_idx = words_indexer.elements_to_index(train_data)\n",
    "        train_tags_idx = tags_indexer.elements_to_index(train_tags)\n",
    "        \n",
    "        # Shuffle data\n",
    "        indices = list(range(len(train_data)))\n",
    "        random.shuffle(indices)\n",
    "        train_data_idx = [train_data_idx[i] for i in indices]\n",
    "        train_tags_idx = [train_tags_idx[i] for i in indices]\n",
    "        \n",
    "        for i in tqdm(range(0, len(train_data), batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "            batch_indices = indices[i:i+batch_size]\n",
    "            batch_data = [torch.tensor(train_data_idx[j], dtype=torch.long) for j in batch_indices]\n",
    "            batch_tags = [torch.tensor(train_tags_idx[j], dtype=torch.long) for j in batch_indices]\n",
    "            \n",
    "            # Pad sequences\n",
    "            batch_data_padded = pad_sequence(batch_data, batch_first=True, padding_value=0).to(device)\n",
    "            batch_tags_padded = pad_sequence(batch_tags, batch_first=True, padding_value=0).to(device)\n",
    "            mask = (batch_data_padded != 0).to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss = model(batch_data_padded, batch_tags_padded, mask=mask)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        scheduler.step()\n",
    "        losses.append(total_loss / num_batches if num_batches > 0 else total_loss)\n",
    "        \n",
    "        # Training metrics\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, tags in zip(train_data_idx, train_tags_idx):\n",
    "                data_tensor = torch.tensor([data], dtype=torch.long).to(device)\n",
    "                mask = (data_tensor != 0).to(device)\n",
    "                pred = model(data_tensor, mask=mask)[0]\n",
    "                metric_handler.update([pred], [tags])\n",
    "            metric_handler.collect()\n",
    "        \n",
    "        # Validation metrics\n",
    "        valid_metric.metrics_dict.clear()\n",
    "        with torch.no_grad():\n",
    "            for data, tags in zip(words_indexer.elements_to_index(valid_data), \n",
    "                                tags_indexer.elements_to_index(valid_tags)):\n",
    "                data_tensor = torch.tensor([data], dtype=torch.long).to(device)\n",
    "                mask = (data_tensor != 0).to(device)\n",
    "                pred = model(data_tensor, mask=mask)[0]\n",
    "                valid_metric.update([pred], [tags])\n",
    "            valid_metric.collect()\n",
    "        \n",
    "        current_f1 = valid_metric.metrics_dict['f1'][-1]\n",
    "        logger.info(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        logger.info(f\"Loss: {losses[-1]:.4f}\")\n",
    "        logger.info(f\"Train - Precision: {metric_handler.metrics_dict['precision'][-1]:.4f}, \"\n",
    "                   f\"Recall: {metric_handler.metrics_dict['recall'][-1]:.4f}, \"\n",
    "                   f\"F1: {metric_handler.metrics_dict['f1'][-1]:.4f}\")\n",
    "        logger.info(f\"Valid - Precision: {valid_metric.metrics_dict['precision'][-1]:.4f}, \"\n",
    "                   f\"Recall: {valid_metric.metrics_dict['recall'][-1]:.4f}, \"\n",
    "                   f\"F1: {valid_metric.metrics_dict['f1'][-1]:.4f}\")\n",
    "        \n",
    "        # Checkpointing\n",
    "        checkpoint_path = os.path.join(checkpoint_dir, f'epoch_{epoch+1}.pt')\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "        logger.info(f\"Saved checkpoint to {checkpoint_path}\")\n",
    "        \n",
    "        # Early stopping\n",
    "        if current_f1 > best_f1:\n",
    "            best_f1 = current_f1\n",
    "            best_model_state = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                logger.info(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    \n",
    "    return model, metric_handler, valid_metric, losses\n",
    "\n",
    "# Visualization function\n",
    "def build_training_visualization(name, train_metrics, losses, valid_metrics, output_path):\n",
    "    \"\"\"Create training and validation metrics visualization.\"\"\"\n",
    "    logger.info(f\"Saving visualization to {output_path}\")\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.plot(losses, label='Loss', color='#1f77b4')\n",
    "    ax1.set_title(f'{name} - Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    \n",
    "    for metric in ['precision', 'recall', 'f1']:\n",
    "        ax2.plot(train_metrics[metric], label=f'Train {metric}', color=f'#{hash(metric) % 0xFFFFFF:06x}')\n",
    "        ax2.plot(valid_metrics[metric], label=f'Valid {metric}', linestyle='--')\n",
    "    ax2.set_title(f'{name} - Metrics')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Score')\n",
    "    ax2.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(output_path)\n",
    "    plt.close()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # File paths\n",
    "    TRAIN_PATH = 'train.txt'\n",
    "    DEV_PATH = 'dev.txt'\n",
    "    TEST_PATH = 'test.txt'\n",
    "    EMBEDDINGS_PATH = 'glove.6B.100d.txt'\n",
    "\n",
    "    # Load data\n",
    "    logger.info(\"Loading data...\")\n",
    "    glove = load_embedding_dict(EMBEDDINGS_PATH, max_words=10000)\n",
    "    words, tags = read_ner_data_from_connl(TRAIN_PATH)\n",
    "    val_words, val_tags = read_ner_data_from_connl(DEV_PATH)\n",
    "    test_words, test_tags = read_ner_data_from_connl(TEST_PATH)\n",
    "    \n",
    "    # Data augmentation\n",
    "    logger.info(\"Augmenting data...\")\n",
    "    aug_words, aug_tags = augment_data(words, tags, glove, augment_factor=0.05)\n",
    "    words.extend(aug_words)\n",
    "    tags.extend(aug_tags)\n",
    "    \n",
    "    data_dict = {\n",
    "        'train': (words, tags),\n",
    "        'dev': (val_words, val_tags),\n",
    "        'test': (test_words, test_tags)\n",
    "    }\n",
    "    \n",
    "    words_indexer = Indexer(words)\n",
    "    tags_indexer = Indexer(tags)\n",
    "    EMBEDDING_DIM = 100\n",
    "    HIDDEN_DIM = 200\n",
    "    \n",
    "    # Simplified hyperparameter search for faster execution\n",
    "    learning_rate = 0.001\n",
    "    dropout = 0.3\n",
    "    \n",
    "    best_f1 = 0\n",
    "    best_model = None\n",
    "    best_strategy = None\n",
    "    \n",
    "    for strat in ['a', 'b', 'c']:\n",
    "        strategy = f\"strategy_{strat}\"\n",
    "        logger.info(f\"Training {strategy} with lr={learning_rate}, dropout={dropout}\")\n",
    "        \n",
    "        model = BiLSTMCRFTagger(\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            hidden_dim=HIDDEN_DIM,\n",
    "            tagset_size=tags_indexer.size(),\n",
    "            embedding_layer=EmbeddingFabric.get_embedding_layer(words_indexer, glove, strategy),\n",
    "            dropout=dropout\n",
    "        ).to(device)\n",
    "        \n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.5)\n",
    "        metric_handler = MetricsHandler(list(tags_indexer.get_element_to_index_dict().values()))\n",
    "        valid_metric = MetricsHandler(list(tags_indexer.get_element_to_index_dict().values()))\n",
    "        \n",
    "        model, train_metrics, valid_metrics, losses = train_model(\n",
    "            model=model,\n",
    "            optimizer=optimizer,\n",
    "            scheduler=scheduler,\n",
    "            data_dict=data_dict,\n",
    "            batch_size=64,\n",
    "            words_indexer=words_indexer,\n",
    "            tags_indexer=tags_indexer,\n",
    "            metric_handler=metric_handler,\n",
    "            valid_metric=valid_metric,\n",
    "            num_epochs=5,\n",
    "            patience=2\n",
    "        )\n",
    "        \n",
    "        # Test set evaluation\n",
    "        test_metrics = MetricsHandler(list(tags_indexer.get_element_to_index_dict().values()))\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, tags in zip(words_indexer.elements_to_index(test_words), \n",
    "                                tags_indexer.elements_to_index(test_tags)):\n",
    "                data_tensor = torch.tensor([data], dtype=torch.long).to(device)\n",
    "                mask = (data_tensor != 0).to(device)\n",
    "                pred = model(data_tensor, mask=mask)[0]\n",
    "                test_metrics.update([pred], [tags])\n",
    "            test_metrics.collect()\n",
    "        \n",
    "        f1_score = test_metrics.metrics_dict['f1'][-1]\n",
    "        logger.info(f\"{strategy} results on test set:\")\n",
    "        for metric in test_metrics.metrics_dict.keys():\n",
    "            logger.info(f\"{metric} - {test_metrics.metrics_dict[metric][-1]:.4f}\")\n",
    "        \n",
    "        if f1_score > best_f1:\n",
    "            best_f1 = f1_score\n",
    "            best_model = copy.deepcopy(model)\n",
    "            best_strategy = strategy\n",
    "        \n",
    "        build_training_visualization(\n",
    "            f\"{strategy}_lr{learning_rate}_drop{dropout}\",\n",
    "            train_metrics.get_metrics(),\n",
    "            losses,\n",
    "            valid_metrics.get_metrics(),\n",
    "            f'{strategy}_lr{learning_rate}_drop{dropout}.png'\n",
    "        )\n",
    "    \n",
    "    logger.info(f\"Best model: {best_strategy} with F1 score: {best_f1:.4f}\")\n",
    "    torch.save(best_model.state_dict(), 'best_model.pt')\n",
    "    logger.info(\"Saved best model to best_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cc8cef-0f80-4c45-b26d-ced34952b6f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
